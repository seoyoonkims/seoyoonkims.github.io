---
title: FORA
layout: default
parent: Paper Review
nav_order: 5
---

### 논문 리뷰  

## FORA: Fast-Forward Caching in Diffusion Transformer Acceleration

---

### **1. Introduction**  

- FORA는 디퓨전 모델의 어텐션이나 MLP 레이어의 중간값들을 캐싱하여 반복적인 연산을 줄인다. 
- 연산 오버헤드를 상당히 많이 줄일 수 있고 DiT 모델에 쉽게 통합할 수 있다.
- 실험적으로 FORA의 성능을 입증하였다. Real-time use에 적합하다.  

---


### **2. Related Work**  


